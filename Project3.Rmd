---
title: "Project3"
author: "Mrudula"
date: "2022-11-01"
output: html_document
---

# Word Frequency in a Novel


Natural Language Processing(NLP) and Text Mining(TM) is a field that covers any problem that is involved with computers interacting with either human speech or written text.

Text Mining, also referred to as Text Data Mining is the process of deriving information from a text. It is a process of gathering unstructured data and extracting useful information from a large volume of databases.

The process of Text Mining is as follows:

*  Gathering unstructured information from various sources like plain text, web pages, PDF records.

*  Pre-processing and Data cleaning tasks are performed to eliminate inconsistency from the data.

*  Pattern analysis - description of solutions for problems in various domains; is implemented.

*  Information processed in these steps is utilized to extract important data for convenient decision-making process.

Word Frequency models are often used as a starting point or a solution to a problem related to text mining. Word Frequency models are also called Bag of Words models. These models count the number of times a word appeared in a document. In other words, it counts the frequency of a word.

## Setting up the environment

Install and load the libraries needed for the project.

```{r loading libraries, message=FALSE}
library(tidyverse)
library(tm)
library(wordcloud2)
```

*  The `tidyverse` package is a collection of R packages, that is designed for data science.

*  `tm` is a package used for text mining.

*  `wordcloud2` is used for generating word clouds. It has slightly different design from the `wordcloud` package.


## Gathering the data

Choose the text document that we are planning on using for text mining. For this project, I chose to take a classic novel - Pride and Prejudice by Jane Austen.

The digitalized version of this book is available on Project Gutenberg. It is a library of over 60,000 free eBooks. We can choose from different  versions; from where we can either download or read online.

First, I'll assign the link to a variable.

```{r link}
link <- "https://www.gutenberg.org/files/1342/1342-0.txt"
```

Read the lines from the document. Use `read_lines()` function.

```{r read lines}
pride <- read_lines(link, skip = 0, skip_empty_rows = TRUE)
```

The argument `skip` defines the number of lines to skip before starting to read the data. `skip_empty_rows` is used to let the function know if the blank rows should be ignored or not. If `TRUE`, the blank rows will not be represented, and if `FALSE`, they will be represented by `NA` values in the columns.


Since it is a novel, there will be a lot of special characters like quotation marks and exclamation marks. We'll remove them, to remove any kinds of inconsistencies. Use `gsub()` function to substitute those non-alphanumeric characters with space.

```{r remove special characters}
pride1 <- gsub('[^[:alnum:]]', ' ', pride)
```

`^[:alnum:]` is the parameter that removes the non-alphanumeric characters.


## Text Pre-processing and Text cleaning

The first step for pre-processing the text is to convert the text into a corpus. A corpus is collection of documents. 

```{r corpus}
book <- Corpus(VectorSource(pride1))
```

`VectorSource()` is a predefined source, provided by the `tm` package. It only accepts character vectors. 

Once the corpus is created, we can proceed with the text cleaning.

Any transformations are done via the `tm_map()` function. It maps a function to all the elements of the corpus.

*  Remove Punctuation: Remove everything that is not a number or a letter.

```{r remove punctuation, warning=FALSE}
pride_clean <- tm_map(book, removePunctuation)
```

*  Convert to lowercase: R is case-sensitive. Which means 'a' and 'A' are treated differently. To treat them as same, we convert the words to lowercase.

```{r convert to lowercase, warning=FALSE}
pride_clean <- tm_map(pride_clean, content_transformer(tolower))
```

`content_transformer()` is a function that modifies the content of an object.

*  Remove numbers: If the numbers are not needed in the analysis, we can remove the numbers. In this project, we don't need them, so let's remove them.

```{r remove numbers, warning=FALSE}
pride_clean <- tm_map(pride_clean, removeNumbers)
```

*  Remove stop words: Stop words are the words that are used so frequently that they do not have any value in the sentence and hence are not useful for Word Frequency models.

```{r stop words}
n_distinct(stopwords('SMART'))

head(stopwords('SMART'), n = 10)
```

We'll take the English stop words from the `SMART` information retrieval system. The first line of code shows the number of stop words in the `SMART` lexicon. And the second line of code shows the first ten words.

Now we'll proceed to remove the stop words.

```{r remove stop words, warning=FALSE}
pride_clean <- tm_map(pride_clean, removeWords, c(stopwords('SMART'), 'chapter'))
```

I'm removing another word, 'chapter'. Because the source from which I took this data from, has links to each chapter. And it is all in the same page. There are titles for each chapter as well. To make sure it does not interfere with the word frequency of the novel, I'm removing the word. 

## Creating Term-Document Matrix

After text pre-processing is done, we need to convert the cleaned file into a term-document matrix.

The Term-Document Matrix is a method of representing the data. In other words, the textual data is represented in the form of a matrix. The rows of the matrix represent the sentences from the data and the columns represent a word.

Use `TermDocumentMatrix()` to convert the data into a term-document matrix. 

```{r tdm}
tdm <- TermDocumentMatrix(pride_clean)
tdm
```

As we can see, the sparsity of the matrix is 100%. A sparse matrix is a matrix that contains mostly zero values. The sparsity of the matrix is generally calculated as `count of zero elements / total elements` in a matrix. 

We need to remove the sparse terms, even if it's a little. The sparsity threshold is set as `sparse = 0.99`; which means it removes only terms that are more sparse than 0.99.

```{r remove sparse terms}
tdm2 <- removeSparseTerms(tdm, sparse = 0.99)
tdm2
```

The sparsity of the matrix decreased to 98%. After removing the zero terms the terms remaining are 28.

We'll proceed and convert it into a normal matrix, where each term is arranged in a two-dimensional rectangular layout.

```{r matrix}
mat <- as.matrix(tdm2)
```

Vectorize the matrix. Use `rowSums()` to calculate the number of times a word occurred in the matrix. Sort them in descending order.

```{r vectorization}
vec <- sort(rowSums(mat), decreasing = TRUE)
```

Finally, convert the vector into a data frame or a tibble.

```{r df}
pride_and_prejudice <- tibble(term = names(vec), freq = vec)

print(pride_and_prejudice, n = 28)
```


## Visualizations

First, we'll generate a word cloud.

```{r word cloud}
wordcloud2(data = pride_and_prejudice, size = 0.7, color = 'random-light',
           backgroundColor = '#281E5D', shape = 'circle')
```

The `size` refers to the size of the font. Default font size is 1. However, the larger the font size the bigger the count of the word. `color` indicates the color of the word. `backgroundColor` is the color of the background. `shape` indicates the shape of the 'cloud' to draw.

We'll now create a column chart. We'll make use of `ggplot2` for this.

```{r col chart}
pride_and_prejudice %>%
  head(10) %>%
  ggplot() + 
  geom_col(mapping = aes(x = term, y = freq), color = "black", fill = "#610C04",
           position = 'dodge') +
  geom_text(aes(x = term, y = freq, label = freq), vjust = 0.3, hjust = 1.2,
            color = "white", size = 3.0) +
  labs(title = "Most frequently used words") +
  coord_flip()
```


## Summary and Conclusion

*  The term 'Elizabeth' is the word that appeared the most in the novel, with a frequency of 634. Second is 'Darcy' with 418. As expected, because they are the main characters.

*  Word Frequency models help us find the words with highest frequency and lowest frequency. 


# Thank You.